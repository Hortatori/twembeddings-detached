standard :
# these options are applied to all models
  batch_size : 8 # Nearest neighbor search is done in batch. A smaller batch will yield better results but is slower
  hashtag_split: True # split hashtags on capital letters,
  svd : False # turn sparse tf-idf features into dense ones using Singular Value Decomposition
  binary : True #
  tfidf_weights: False # use tf-idf weights when averaging Word2Vec vectors
  save : True # save features in the form of a .npy or .npz (for sparse vectors) file
  save_results: True # save results in a .csv file
  lang : "fr"
  dataset : "data/event2012.tsv"
  annotation : "annotated"
  text+ : False
  clustering : "FSD"
  daily : "False"

# You may configure specific options for each model using the following fields to overwrite standard parameters
tfidf_all_tweets :
  save : False
  threshold :
     - 0.6
     - 0.65
     - 0.70
     - 0.75
     - 0.8
  remove_mentions : True
tfidf_dataset :
  save : False
  threshold :
    - 0.55
    - 0.6
    - 0.65
    - 0.7
    - 0.75
  remove_mentions : True
w2v_gnews_en :
  threshold :
    - 0.20
    - 0.25
    - 0.30
    - 0.35
    - 0.40
  remove_mentions : True
sbert_nli_sts:
  threshold:
    - 0.3
    - 0.35
    - 0.4
    - 0.45
    - 0.5
  remove_mentions : False
sbert:
  threshold:
    # - 0.30
    # - 0.45
    # - 0.50
    # - 0.52
    # - 0.54 # best f1 score with FSD + sbert on all dataset
    # - 0.56
    # - 0.60
    # - 0.65
    - 0.70 # best f1 score with agglomerativeclustering + sbert daily
    # - 0.80
  remove_mentions : False
  # sub_model : "Lajavaness/sentence-camembert-large"
  # sub_model : "dangvantuan/sentence-camembert-large"
  # sub_model : "almanach/camemberta-base" PB : produit toujours le même résultat
  # sub_model : "almanach/camembert-large" div zero error très tôt
  # sub_model : "dangvantuan/sentence-camembert-base"
  # sub_model : "Wissam42/sentence-croissant-llm-base"
  # sub_model : "sentence-transformers/distiluse-base-multilingual-cased-v1"
  # sub_model : "/data/llama-2/7b_hf"
  # sub_model : "paraphrase-multilingual-MiniLM-L12-v2"
  # sub_model : "Salesforce/SFR-Embedding-2_R" out of memory
  # sub_model : "WhereIsAI/UAE-Large-V1"
  sub_model : "sentence-transformers/all-mpnet-base-v2"
  # sub_model : "intfloat/multilingual-e5-large-instruct" error float division by zero et toujours le meme nb de clusters
bert:
  threshold:
    - 0.02
    - 0.03
    - 0.04
    - 0.05
    - 0.06
  remove_mentions : False
elmo:
  threshold:
    - 0.04
    - 0.06
    - 0.08
    - 0.1
    - 0.2
  remove_mentions : False
use:
  threshold:
    - 0.3
    - 0.4
    - 0.45
    - 0.5
    - 0.55
  remove_mentions : False
